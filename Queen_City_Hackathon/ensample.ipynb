{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 12})\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_to_csv(y_pred):\n",
    "    \"\"\"\n",
    "    Use this function to save your prediction result to a csv file.\n",
    "    The resulting csv file is named as [team_name].csv\n",
    "\n",
    "    :param y_pred: an array or a pandas series that follows the SAME index order as in the testing data\n",
    "    \"\"\"\n",
    "    pd.DataFrame(dict(\n",
    "        target=y_pred\n",
    "    )).to_csv('predictions.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(thresh = 0.85):\n",
    "\n",
    "    #### Read Train and Test Data\n",
    "    training_data = pd.read_csv('/home/rajneesh/Desktop/Queen_City_Hackathon/training.csv', index_col=0)\n",
    "    testing_data = pd.read_csv('/home/rajneesh/Desktop/Queen_City_Hackathon/testing.csv', index_col=0)\n",
    "\n",
    "    ### Check for the co relared columns \n",
    "\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = training_data.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    columns_to_drop = [column for column in upper.columns if any(upper[column] > thresh)]\n",
    "    if 'target' in columns_to_drop:\n",
    "        columns_to_drop.remove('target')\n",
    "\n",
    "    ### Drop all the co-related columns\n",
    "    data_nodups_df = training_data.drop(columns=columns_to_drop)\n",
    "    \n",
    "    #### Data Preparation\n",
    "    train_df = data_nodups_df.fillna(data_nodups_df.mean())\n",
    "\n",
    "    #y = training_data['target']\n",
    "    #X = data_nonull_df.drop(columns=['target'])\n",
    "\n",
    "    test_nodups_df = testing_data.drop(columns=columns_to_drop)\n",
    "    test_df = test_nodups_df.fillna(data_nodups_df.mean())\n",
    "    \n",
    "    return (train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df, test_df) = prepare_data(thresh=0.85)\n",
    "y = train_df['target']\n",
    "X = train_df.drop(columns = ['target'])\n",
    "\n",
    "\n",
    "st_scale = StandardScaler()\n",
    "X = st_scale.fit_transform(X)\n",
    "test_df = st_scale.fit_transform(test_df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajneesh/miniconda3/envs/py36/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.08\n"
     ]
    }
   ],
   "source": [
    "rf_pipe = Pipeline([('rforest', RandomForestRegressor())])\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(rf_pipe.score(X_test, y_test )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.10\n"
     ]
    }
   ],
   "source": [
    "gbr_pipe = Pipeline([('rforest', GradientBoostingRegressor())])\n",
    "\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(gbr_pipe.score(X_test, y_test )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.09\n"
     ]
    }
   ],
   "source": [
    "abr_pipe = Pipeline([('rforest', AdaBoostRegressor())])\n",
    "\n",
    "abr_pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(abr_pipe.score(X_test, y_test )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.10\n"
     ]
    }
   ],
   "source": [
    "bg_pipe = Pipeline([('rforest', BaggingRegressor())])\n",
    "\n",
    "bg_pipe.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(bg_pipe.score(X_test, y_test )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4650672391751234\n",
      "1.4034649813744355\n",
      "1.3942168818264178\n",
      "1.381505006356169\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, rf_pipe.predict(X_test)))\n",
    "print(mean_squared_error(y_test, gbr_pipe.predict(X_test)))\n",
    "print(mean_squared_error(y_test, abr_pipe.predict(X_test)))\n",
    "print(mean_squared_error(y_test, bg_pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajneesh/miniconda3/envs/py36/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:53:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1572314959925/work/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingRegressor(estimators=[('knn',\n",
       "                             KNeighborsRegressor(algorithm='auto', leaf_size=30,\n",
       "                                                 metric='minkowski',\n",
       "                                                 metric_params=None,\n",
       "                                                 n_jobs=None, n_neighbors=3,\n",
       "                                                 p=2, weights='uniform')),\n",
       "                            ('ridge',\n",
       "                             Ridge(alpha=10, copy_X=True, fit_intercept=True,\n",
       "                                   max_iter=None, normalize=False,\n",
       "                                   random_state=None, solver='auto',\n",
       "                                   tol=0.001)),\n",
       "                            ('lasso',\n",
       "                             Lasso(alpha=10, copy_X=True, fit_intercep...\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_state=0, reg_alpha=0,\n",
       "                                          reg_lambda=1, scale_pos_weight=1,\n",
       "                                          seed=None, silent=None, subsample=1,\n",
       "                                          verbosity=1))],\n",
       "                n_jobs=None, weights=None)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1, 'max_depth': 5, 'alpha': 10}\n",
    "voting_clf = VotingRegressor([('knn', KNeighborsRegressor(n_neighbors=3)), ('ridge', Ridge(alpha=10)), ('lasso', Lasso(alpha=10)), ('xgb', xgb.XGBRegressor())])\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.13\n",
      "1.3333612642158403\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.2f}\".format(voting_clf.score(X_test, y_test )))\n",
    "print(mean_squared_error(y_test, voting_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajneesh/miniconda3/envs/py36/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/rajneesh/miniconda3/envs/py36/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:04] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1572314959925/work/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "voting_clf.fit(X, y)\n",
    "y_pred = voting_clf.predict(test_df)\n",
    "save_prediction_to_csv(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0164\n",
      "Variance Score: 0.7910\n",
      "Test score: 0.79\n",
      "1.3978444353592037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rajneesh/miniconda3/envs/py36/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "lr = LinearRegression()\n",
    "svr_lin = SVR(kernel='linear')\n",
    "ridge = Ridge(random_state=1)\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "\n",
    "stregr = StackingRegressor(regressors=[svr_lin, lr, ridge], meta_regressor=svr_rbf)\n",
    "\n",
    "# Training the stacking classifier\n",
    "\n",
    "stregr.fit(X_train, y_train)\n",
    "stregr.predict(X_test)\n",
    "\n",
    "# Evaluate and visualize the fit\n",
    "\n",
    "print(\"Mean Squared Error: %.4f\" % np.mean((stregr.predict(X_train) - y_train) ** 2))\n",
    "print('Variance Score: %.4f' % stregr.score(X_train, y_train))\n",
    "\n",
    "print(\"Test score: {:.2f}\".format(stregr.score(X_train, y_train)))\n",
    "print(mean_squared_error(y_test, stregr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
